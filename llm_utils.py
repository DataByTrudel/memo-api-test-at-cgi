# llm_utils.py

def prepare_llm_input(question: str, ask_response: dict) -> dict:
    """
    Transforms the /ask API response into a CPF-compatible input format for LLM synthesis.
    """
    documents = []

    for result in ask_response.get("results", []):
        documents.append({
            "filename": result.get("metadata_storage_path", "unknown").split("/")[-1],
            "page": 1,  # Placeholder
            "content": result.get("content_preview", ""),
            "supersedes": False
        })

    return {
        "question": question,
        "documents": documents,
        "instructions": {
            "honor_supersession": True,
            "output_format": "acheron_json"
        }
    }

def mock_gpt_call(llm_input: dict) -> dict:
    """
    Mock GPT response based on CPF interpretive structure.
    """
    return {
        "intent": "interpretive",
        "summary": "Based on the retrieved content, COLA is determined by CPI-based calculations outlined in recent memos.",
        "citations": [
            {
                "filename": doc["filename"],
                "page": doc["page"],
                "rationale": "Contains explanation of COLA formula"
            }
            for doc in llm_input.get("documents", [])
        ],
        "why_these": "Explanation generated by cgi-a: Used documents that mentioned CPI or annual adjustment process"
    }
