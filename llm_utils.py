# llm_utils.py
import openai
import os

openai.api_type = "azure"
openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
openai.api_key = os.getenv("AZURE_OPENAI_KEY")
openai.api_version = "2023-07-01-preview"  # or newer if youâ€™ve enabled it

def prepare_llm_input(question: str, ask_response: dict) -> dict:
    documents = []
    for result in ask_response.get("results", []):
        documents.append({
            "filename": result.get("metadata_storage_path", "unknown").split("/")[-1],
            "page": 1,
            "content": result.get("content_preview", "")
        })

    return {
        "question": question,
        "documents": documents
    }


    return {
        "question": question,
        "documents": documents,
        "instructions": {
            "honor_supersession": True,
            "output_format": "acheron_json"
        }
    }

def mock_gpt_call(llm_input: dict) -> dict:
    """
    Mock GPT response based on CPF interpretive structure.
    """
    return {
        "intent": "interpretive",
        "summary": "Based on the retrieved content, COLA is determined by CPI-based calculations outlined in recent memos.",
        "citations": [
            {
                "filename": doc["filename"],
                "page": doc["page"],
                "rationale": "Contains explanation of COLA formula"
            }
            for doc in llm_input.get("documents", [])
        ],
        "why_these": "Explanation generated by cgi-a: Used documents that mentioned CPI or annual adjustment process"
    }

def load_prompt_template() -> str:
    with open("acheron_prompt.txt", "r", encoding="utf-8") as f:
        return f.read()

from openai import AzureOpenAI
import os

client = AzureOpenAI(
    api_key=os.getenv("AZURE_OPENAI_KEY"),
    api_version="2023-07-01-preview",
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
)

def call_gpt(llm_input: dict) -> dict:
    prompt_template = load_prompt_template()

    doc_block = "\n\n".join(
        f"{doc['filename']} (p. {doc['page']}):\n{doc['content']}"
        for doc in llm_input["documents"]
    )

    full_prompt = f"{prompt_template}\n\nUser question: {llm_input['question']}\n\nRetrieved memo pages:\n{doc_block}"

    response = client.chat.completions.create(
        model="gpt-4o",  # Use your Azure deployment name here
        messages=[{"role": "system", "content": full_prompt}],
        temperature=0.3
    )

    return response.choices[0].message.content


